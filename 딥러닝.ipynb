{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPM6N2K3ZOeXPDeNoGmOM7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cho-mindong/study_ai_pre-cousre/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FCx2HVyon8k",
        "colab_type": "text"
      },
      "source": [
        "#딥러닝\n",
        "\n",
        "1. 딥러닝의 유래\n",
        "2. 퍼셉트론(Perceptron)\n",
        "3. AND OR XOR 문제\n",
        "4. 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z30_7vz1oubA",
        "colab_type": "text"
      },
      "source": [
        "##1. 딥러닝의 유래\n",
        "1. 맥컬록과 피츠는 뉴런에 관한 연구: 신경세포를 이진 출력을 내는 간단한 논리 회로로 표현\n",
        "2. 헵의 학습 규칙:\n",
        "\n",
        "  인간두뇌의 작용은 개별 신경세포에 이뤄지는 것이 아닌 그들간의 연결 강도로 정해진다. 우리의 두뇌가 신경망으로 활동하고 있음을 설명. 시냅스는 연결되어 있는 두 뉴런이 반복적으로 활성화 되면 연결강도가 강화된다. 신경망모델들의 학습규칙에 토대가 된다. - ___요약하자면 두 뉴런이 활성화 할때마다 가중치가 증가하는 경향이 있다.___\n",
        "3. 프랭크로 젠블렛은 이를 기반으로 페셉트론(perceptron)학습 개념 발표.\n",
        "\n",
        "4. XOR문제로 인해 인공신경망의 겨울 도래\n",
        "5. 다층 퍼셉트론에 대한 연구와 역전파 알고리즘 발표\n",
        "6. 딥러닝의 발전\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gNtuAdk4v9v",
        "colab_type": "text"
      },
      "source": [
        "##2. 퍼셉트론(Perceptron)\n",
        "퍼셉트론은 가장 간단한 인공 신경망 구조. \n",
        "퍼셉트론은 인공뉴런을 기반으로 한다.\n",
        "\n",
        "1. __인공뉴런__의 구조: \n",
        "\n",
        "  다수의 __입력뉴런(그림을 보면 입력층에 있는 각각의 노드를 말한다.)__에서 입력값을 받아 각각의 가중치와 곱해져 모든 입력값*가중치의 합이 임계값에 따라(활성함수) __하나의 출력값__을 출력한다.\n",
        "실제 뉴런의 구조와 매우 비슷하다.(인공뉴런구조그림: 핸즈온 머신러닝, p357)\n",
        "\n",
        "2. __단층퍼셉트론의 구조__:\n",
        "\n",
        " 편향 뉴런을 더해진 입력층과 출력층으로 구성하고 있다. 그리고 출력층에 활성화 함수가 있어 입력값과 가중치의 곱의 합을 활성화 함수에 넣어 출력값을 출력한다.\n",
        "(단층퍼셉트론 구조 그림: 핸즈온 머신러닝 , p.358)\n",
        "\n",
        "3. __다층퍼셉트론의 구조__:\n",
        "\n",
        "  단층퍼셉트론의 구조에 하나 이상의 은닉층을 입력층과 출력층사이에 둔 구조이다. 출력층을 제외한 모든 층에는 편향뉴런이 있다. ___은닉층은 그 전의 입력층 또는 은닉층의 출력층으로 볼수 있다. 따라서 각 은닉층은 그 전 층의 입력값*가중치의 합을 활성화 함수에 넣어 얻은 출력값을 입력값으로 받는다.___ (다층퍼셉트론 구조 그림: 핸즈온 머신러닝, p361)\n",
        "\n",
        "\n",
        "층=레이어(layor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijGAjYoT-N7q",
        "colab_type": "text"
      },
      "source": [
        "그럼 이제는 퍼셉트론이 어떻게 작동하는지 알아 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw5-_Z8crwCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.set_random_seed(2020)\n",
        "x=1  #입력값=1\n",
        "y=0  #원하는 출력값=0\n",
        "w=tf.random.normal([1],0,1)  #가중치는 정규분포에 무작위 값을 설정\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGa1VcufsZtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def sigmoid(x):           #활성함수에는 여러가지 있지만 여기에서는 시그모이드함수사용\n",
        "  return 1/(1+math.exp(-x))   #f(x)=1/1+e^x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXmQK0f2s5du",
        "colab_type": "code",
        "outputId": "07df407b-4a32-4a3a-aa52-40408487ee22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output=sigmoid(x*w)   #y=f(x*w)=1/1+e^(x*w)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47477188589261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRW685oftL9e",
        "colab_type": "text"
      },
      "source": [
        "우리가 원하는 출력값은 0인데 0과는 차이가 많이 나는 출력값이 나와버렸다.(시그모이드 함수의 결과값은 0~1이니깐 차이가 큰편이다.)\n",
        "\n",
        "이는 초기화한 가중치가 적절하지 않다는 것을 알수 있다.\n",
        "\n",
        "___그럼 우리는 오차를 측정하는 지표를 정해 오차를 측정하고 오차측정값이 0에 가까워 지도록 가중치를 변화시켜야 될것이다.___\n",
        "\n",
        "여기서 오차를 측정하는 지표(error)는 \n",
        "\n",
        "__(원하는 출력값)-(출력값)__으로 정하자.\n",
        "\n",
        "그리고 __경사하강법__을 이용해 가중치를 변화 시키자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPt2IN03uDrw",
        "colab_type": "code",
        "outputId": "f7362746-dc17-4ea1-af01-2b04d33b9df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for i in range(1000):       #가중치조정횟수를 충분히 많게 설정\n",
        "  output=sigmoid(x*w)     \n",
        "  error=y-output        #오차측정(error)설정\n",
        "  w=w+x*0.1*error      #경사하강법, 0.1은 학습률\n",
        "\n",
        "  if i %100==99:\n",
        "    print(\"학습 횟수:\", i, \"Error:\", error, \"결과값:\", output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 횟수: 99 Error: -0.10010598284299604 결과값: 0.10010598284299604\n",
            "학습 횟수: 199 Error: -0.05178399422833116 결과값: 0.05178399422833116\n",
            "학습 횟수: 299 Error: -0.034590451977903586 결과값: 0.034590451977903586\n",
            "학습 횟수: 399 Error: -0.02588962752851373 결과값: 0.02588962752851373\n",
            "학습 횟수: 499 Error: -0.020658699939863617 결과값: 0.020658699939863617\n",
            "학습 횟수: 599 Error: -0.017174253993457355 결과값: 0.017174253993457355\n",
            "학습 횟수: 699 Error: -0.014689506449480992 결과값: 0.014689506449480992\n",
            "학습 횟수: 799 Error: -0.012829497265431342 결과값: 0.012829497265431342\n",
            "학습 횟수: 899 Error: -0.011385568271837804 결과값: 0.011385568271837804\n",
            "학습 횟수: 999 Error: -0.010232493309882492 결과값: 0.010232493309882492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73cXTQtJv0Mq",
        "colab_type": "text"
      },
      "source": [
        "오차가 점점 줄어들고 결과값이 원하는 출력값(0)과 비슷해지고 있다.\n",
        "\n",
        "그럼 이번에는 입력값을 0, 원하는 출력값을 1로 두자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pJr35v4v8hV",
        "colab_type": "code",
        "outputId": "08621120-f917-4993-d8f1-472fad5471aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "x=0   #입력값을 0\n",
        "y=1   #원하는 출력값을 1\n",
        "w=tf.random.normal([1],0,1)   #나머지는 다 동일하게\n",
        "\n",
        "for i in range(1000):\n",
        "  output=sigmoid(x*w)\n",
        "  error=y-output\n",
        "  w=w+x*0.1*error\n",
        "\n",
        "  if i %100==99:\n",
        "    print(\"학습 횟수:\", i, \"Error:\", error, \"결과값:\", output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 횟수: 99 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 199 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 299 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 399 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 499 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 599 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 699 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 799 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 899 Error: 0.5 결과값: 0.5\n",
            "학습 횟수: 999 Error: 0.5 결과값: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B6Fw-vrwx0d",
        "colab_type": "text"
      },
      "source": [
        "오차가 똑같고 결과값이 변화하지 않는다\n",
        "\n",
        "왜냐하면\n",
        "        \n",
        "      w=w+x*0.1*error  #x=0 이어서 w=w 즉, 똑같아짐\n",
        "\n",
        "그래서 가중치가 조정이 안되었던 것.\n",
        "\n",
        "\n",
        "이러한 경우를 방지하고자 ___편향__이라는 개념이 등장한 것이다.\n",
        "\n",
        "__편향__이란 \n",
        "\n",
        "한쪽으로 치우쳐진 고정값.\n",
        "\n",
        "입력뉴런을 하나 더 만든다. 보통 이 뉴런을 편향뉴런이라고 한다. 편향뉴런에 고정값 1을 주입해 1*편향값(b, 편향뉴런의 가중치라고 보면 됨) 다른 입력값들과 같이 계산되게 한다. \n",
        "\n",
        "이렇게 함으로써 입력값으로 0을 입력받아도 편향값이 존재하기 때문에 가중치가 변화하게 되고 오차가 점점 줄어들며 결국 결과값이 원하는 결과값과 비슷해진다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDvlVF1_Rji",
        "colab_type": "text"
      },
      "source": [
        "그럼 편향값을 퍼셉트론에 집어넣자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zos7hBx4yYEg",
        "colab_type": "code",
        "outputId": "ba866648-adb1-4914-e622-abf4bc0792fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "x=0   #입력값을 0\n",
        "y=1   #원하는 출력값을 1\n",
        "w=tf.random.normal([1],0,1)   \n",
        "b=tf.random.normal([1],0,1)  #편향값도 난수로 초기화\n",
        "\n",
        "for i in range(1000):\n",
        "  output=sigmoid(x*w+1*b)   #편향값이 입력값과가중치곱에 더해진다\n",
        "  error=y-output\n",
        "  w=w+x*0.1*error\n",
        "  b=b+1*0.1*error            #편향값도 계속 조정된다\n",
        "\n",
        "  if i %100==99:\n",
        "    print(\"학습 횟수:\", i, \"Error:\", error, \"결과값:\", output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 횟수: 99 Error: 0.11238258794086264 결과값: 0.8876174120591374\n",
            "학습 횟수: 199 Error: 0.055071863836639534 결과값: 0.9449281361633605\n",
            "학습 횟수: 299 Error: 0.036054548409353404 결과값: 0.9639454515906466\n",
            "학습 횟수: 399 Error: 0.026708631840398844 결과값: 0.9732913681596012\n",
            "학습 횟수: 499 Error: 0.02117969038866041 결과값: 0.9788203096113396\n",
            "학습 횟수: 599 Error: 0.017534051963307373 결과값: 0.9824659480366926\n",
            "학습 횟수: 699 Error: 0.01495259801362292 결과값: 0.9850474019863771\n",
            "학습 횟수: 799 Error: 0.013030120422014457 결과값: 0.9869698795779855\n",
            "학습 횟수: 899 Error: 0.011543493497685131 결과값: 0.9884565065023149\n",
            "학습 횟수: 999 Error: 0.01036000375513546 결과값: 0.9896399962448645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5udWlXsd0G70",
        "colab_type": "text"
      },
      "source": [
        "반복할 수록 결과값이 원하는 결과값(1)과 비슷해진다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijortkhw0ROr",
        "colab_type": "text"
      },
      "source": [
        "##3. AND OR XOR 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpL6qGL0VhX",
        "colab_type": "text"
      },
      "source": [
        "참=1\n",
        "거짓=0\n",
        "\n",
        "1. AND: 두개의 입력값이 모두 참일때만 출력값이 참이고 입력값중 하나라도 거짓이 있으면 출력값이 거짓이다.\n",
        "\n",
        "2. OR: 두개의 입력값이 모두 거짓일때만 출력값이 거짓이고 입력값중 하나라도 참이 있으면 출력값은 참이다.\n",
        "\n",
        "3. XOR: 두개의 입력값이 서로 다를때 출력값은 참이고 같으면 출력값은 거짓이다.\n",
        "\n",
        "두 입력값을 각각 x,y축으로 두고 일어날수있는 모든 사례를 좌표평면에 점을 찍어보자. 그런 다음 참의 레이블을 가진 점과 거짓의 레이블을 가진 점을 구분해보자. 결정경계로.\n",
        "\n",
        "AND 와 OR 문제는 선형결정경계로 분류할 수 있다. 이건 단층 퍼셉트론으로 해결가능하다.\n",
        "\n",
        "XOR문제는 선형결정경계로 분류할 수 없다. 이건 단층 퍼셉트론으로 해결하지 못했다.\n",
        "\n",
        "물론 다층 퍼셉트론을 활용하면 XOR 문제해결가능하다.\n",
        "\n",
        "하지만 1969년 연구진들은 퍼셉트론이 이런 간단한 XOR문제도 풀수 없다고 여기고 단순선형분류기라고 했다. 이는 곧 인공신경망 연구의 겨울을 불러오게 되었다\n",
        "\n",
        "\n",
        "1986년 다층퍼셉트론 즉, 은닉층을 활용하게 되면 선형분류선을 여러개 그리는 효과를 얻음으로서 다층퍼셉트론으로 XOR문제를 해결가능하게 했다. \n",
        "\n",
        "하지만 다층퍼셉트론의 문제는 파라미터의 개수가 많아지면서 적절한 가중치와 편향을 학습하는데 어렵다는 것. \n",
        "\n",
        "하지만 제프리 힌튼은 역전파알고리즘을 제시해 문제를 해결. 이로인해 인공신경망연구에 박차를 가함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oScf-KQJw4LF",
        "colab_type": "text"
      },
      "source": [
        "##4. 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFRl0-5w8oH",
        "colab_type": "text"
      },
      "source": [
        "지금까지 말한 방식대로 심층 신경망(다층 퍼셉트론)을 이용해 출력값을 출력했다. 하지만 당연히 이 출력값도 기대한 출력값과 오차가 날 것이다.\n",
        "이제 우리가 할 일은 \n",
        "1. 다른 머신러닝 알고리즘 처럼 오차 측정 지표(손실함수)를 정한다. (예. 선형회귀- 제곱오차)\n",
        "2. 오차를 줄이는 방식으로 학습할 학습 알고리즘을 정한다.(예. 선형회귀- 경사하강법)\n",
        "3. 학습을 반복해 파라미터를 조정한다. 신경망에서 파라미터는 가중치와 편향이다.\n",
        "\n",
        "하지만 신경망 알고리즘에서는 다른 머신러닝 알고리즘과 다른 점이 있으니 은닉층이 존재한다는 것과 층마다 여러개의 입력값과 출력값이 있다.\n",
        "\n",
        "예를 들자면 어떤 다층 퍼셉트론의 구조는 입력층 --> 은닉층 --> 출력층 이라 하자. \n",
        "\n",
        "이 다층 퍼셉트론의 출력값은 몇개 일까?\n",
        "당연히 출력층에 있는 출력값들일 것이다.\n",
        "___하지만 입력층--> 은닉층 부분만 따로 보면 이 구조에서는 은닉층에 있는 값들이 출력값이다.___\n",
        "\n",
        "___그리고 은닉층 --> 출력층 부분만 따로 보면 이 구조에서는 은닉층이 입력값이고 출력층이 출력값이다.___\n",
        "\n",
        "__이처럼 기준이 어디냐에 따라 입력과 출력이 달라진다.__\n",
        "비교해보자면 다른 머신러닝 알고리즘(ex.선형회귀, 로지스틱회귀 등)의 경우 입력 --> 출력 으로 구성되어 있다.\n",
        "\n",
        "이처럼 출력값들이 두개의 층들(은닉층,출력층)로 구성되어 있어 각 층마다 따로따로 그 전에 있는 층의 파라미터를 조정해 출력값을 각각의 기대한 출력값으로 조정해 나가야한다.\n",
        "\n",
        " 따라서 다층퍼셉트론은 조정해야할 파라미터가 엄청 많다는 것과 각 층을 단계적으로 조정해나가야 한다. 그래서 파라미터를 조정하기가 너무 어렵다.\n",
        "\n",
        "내가 이걸 적어도 나도 잘 이해가 안되는 부분인 것처럼 과거 연구진들도 이 문제를 해결하기위해 많은 노력을 하셨고,\n",
        "\n",
        "드디어 제프리 힌튼이 역전파를 발표해 이런 문제를 타파했다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVD4SNjaSd7b",
        "colab_type": "text"
      },
      "source": [
        "역전파는 뉴런의 가중치를 효율적으로 조정하기 위해 심층 신경망에 거꾸로 오차를 전파해 출력층--> 은닉층 --> 입력층 순으로 그 층의 가중치를 조정한다.\n",
        "\n",
        "###___역전파 알고리즘(입력층--> 은닉층A-->은닉층B-->출력층)___\n",
        "\n",
        "1. 모든 가중치와 편향에 랜덤값을 넣고 신경망을 순전파로 계산(입력층 --> 출력층 즉, 순서대로 계산).\n",
        "\n",
        "2.  손실함수(제곱오차)를 이용해서 출력값과 지도데이터(기대 출력값)의 오차 측정 값을 구한다.\n",
        "\n",
        "3. 출력층과 그 이전 은닉층B만 보자.\n",
        "__각 출력 연결(은닉층B에 있는 가중치들과 편향)이 이 오차에 기여하는 정도를 계산한다. 기여하는 정도를 구하는 방법은 각 가중치들과 편향으로 오차 측정값을 편미분한 값이 크면 오차의 변화량이 크다는 이야기니 클수록 오차의 기여도가 큰 것이다.(ex. dE/dW1=dE/db*2 이면 W1에대한 오차기여도가 b(편향)에 대한 오차 기여도보다 크다.) 이때, 연쇄 법칙을 이용하면 빠르고 정확하게 구할 수 있다.__\n",
        "4. 은닉층B와 은닉층A만 보자. 3단계처럼 똑같이 오차의 기여하는 정도를 계산한다.\n",
        "5. 이런식으로 입력층에 도달할때 까지 계속해 모든 가중치와 편향에 대한 오차 기여정도를 측정한다.\n",
        "7. 경사하강법과 모든 오차 기여정도를 사용해 네트워크에 있는 모든 가중치와 편향을 수정한다.[연쇄법칙으로 구하는 방법](http://naver.me/GVbNGZxd)\n",
        "\n",
        "이처럼 역전파 알고리즘은 미분을 많이해 역방향 미분이라고 한다.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "입력값은 각각의 손실함수 전체를 고려해야 한다. 즉, 특정 입력값에서 손실함수가 최소가 되더라도 손실함수 전체가 최소가 아닐 때 아무 의미 없다는 이야기다. 그럼 각각의 손실 함수 합은 E=E1+E2+E3+...+En로 나타낼 수 있다.\n",
        "\n",
        "이때 손실함수 E가 최솟값일 때 가중치를 어떻게 찾냐면 경사하강법을 사용하니깐 E를 W로 편미분해 dE/dW=0 일때 E의 값을 최솟값으로 구할수있다. \n",
        "\n",
        "하지만 이때 E의 값이 무조건 최솟값은 아니다. 왜냐하면 전역 최솟값(전체에서 최솟값)이 아니고 지역 최솟값(일정부근에서는 최솟값)이기 때문이다.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "텐서플로우같은 라이브러리로 역전파 알고리즘을 사용하면 간단하게 코딩 가능하다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IDCH1j-rl0M",
        "colab_type": "text"
      },
      "source": [
        "__역전파 알고리즘의 문제__\n",
        "\n",
        "기울기 소멸 문제가 있다. \n",
        "\n",
        "역전파 알고리즘으로 학습을 진행하는데 있어 주로 사용된 활성화 함수는 시그모이드와 스프트맥스함수였다. \n",
        "\n",
        "이 두함수는 최종 출력을 결정하는데 있어 합리적인 선택이 가능했지만 출력된 값들이 항상 너무 작은 값을 가지고 있었기에 신경망이 깊어지면 오차의 기울기가 점점 작아져 기울기가 소멸돼 오차를 측정할 수 없게 되고 이로 인해 가중치 조정이 불가능해 진다. \n",
        "\n",
        "제프리힌튼교수님은 다양한 활성화 함수를 제시했고 그중 ___ReLU 활성화 함수___를 활용하게 되면 문제가 해결된다.\n",
        "___ReLU 활성화 함수:___ 입력이 음수일때는 0 출력, 양수일때는 양수값 그대로 출력한다."
      ]
    }
  ]
}