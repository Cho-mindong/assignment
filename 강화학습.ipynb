{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "강화학습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgT/PjLzTRrdn6yGUyQE/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cho-mindong/study_ai_pre-cousre/blob/master/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpHbE2TjdaAY",
        "colab_type": "text"
      },
      "source": [
        "#강화학습\n",
        "\n",
        "1. 강화학습이란?\n",
        "2. 용어 정리\n",
        "3. 강화학습 방법\n",
        "4. 탐험과 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFzkLHwMdP9j",
        "colab_type": "text"
      },
      "source": [
        "##1. 강화학습이란?\n",
        "\n",
        "강화학습은 지도학습과 달리 레이블이 없다. 비지도학습과 달리 데이터만을 기반으로 학습하지도 않는다.\n",
        "\n",
        "강화학습은 에이전트가 환경과 상호작용하며 보상과 패널티(벌점, 보상을 깍아먹는다.)를 받으면서 보상이 최대화하는 쪽으로 에이전트가 학습한다. 즉, 보상을 최대화하는 에이전트의 의사결정전략을 학습하는 것. 또 다른 말로 순차적인 행동(에이전트가 의사결정전략에 의해 하는 행동)들을 알아나가는 방법을 학습.\n",
        "\n",
        "명확한 보상을 설정할 수 있는 문제를 해결하는데 사용된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUnT1BNSrvu0",
        "colab_type": "text"
      },
      "source": [
        "##2. 용어정리\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. 에이전트: 의사를 결정하는 역할\n",
        "2. 환경: 에이전트의 의사결정을 반영해 에이전트에게 그 의사결정이 잘한거라면 보상을 못한거라면 패널티를 준다.\n",
        "3. 정책: 에이전트가 행동을 결정하기 위해 사용하는 알고리즘\n",
        "3. 상태: 에이전트가 의사결정을 하기 위해 사용되는 관측값, 행동, 보상을 말한다.에이전트는 상태를 기반으로 의사결정을 진행한다. 기호로 __S__. 특정(t) 상태값은 기호로 ___st___\n",
        "4. 행동: 에이전트가 의사결정을 통해 취할 수 있는 행동. 현재상태에서 취하는행동을  기호로 ___A___. 특정(t) 상태에서 하는 행동을 기호로 ___at___\n",
        "\n",
        "  이산적인 행동과 연속적인 행동이 있다. 이산적인지 연속적인지는 환경에 따라 결정한다.\n",
        "  - 이산적인 행동을 하는 환경은 에이전트에게 주어지는 행동의 선택지가 있다. 에이전트는 그중하나를 행동한다.\n",
        "  - 연속적인 행동을 하는 환경 같은 경우에는 선택지마다 특정 값을 수치로 입력하게 되고 에이전트는 입력된 값만큼 행동.\n",
        "\n",
        "5. 관측: 에이전트가 환경으로부터 얻는 정보다. 시각적 관측과 수치적 관측으로 구분된다. \n",
        "  - 시각적 관측은 상태의 정보를 이미지로 표현한 것이다. \n",
        "  - 수치적 관측은 상태의 정보를 수치로만 표현한것. ex. [1,2,3,4]\n",
        "\n",
        "6. 보상함수: 에이전트가 특정 상태에서 특정 행동을 했을 때 보상(보상 또는 패널티를 말한다.)을 보상(R)함수로 받게 된다. 에이전트는 이 보상을 통해 학습을 진행하게 된다. \n",
        "\n",
        "현재상태에서 특정(t)행동을 했을 때 얻는 보상의기댓값(R(a,s))을 구해 보자.\n",
        "\n",
        "__R(a,s) = E(기댓값)[ R(t+1)(얻을 수 있는 보상) | S=s(현재상태에서), A=a(현재상태의 행동을 취해서)]__  -> s라는 상태에서 a라는 현재행동을 취해 R이라는 보상을 얻는 기댓값이다. 일반적으로 E(x) 는 기댓값을 뜻한다.\n",
        "\n",
        "다시 식을 정리하면 __R(a,s)=E[R(t+1) | S=s, A=a]__\n",
        "\n",
        "__EX)__   만약 어떤 에이전트가 왼쪽으로 가는 행동을 하면 보상 1을 얻고 나머지 방향으로 가면 보상을 못받을 때, 이때의 보상함수는?\n",
        "\n",
        "R(a,s)= 1 (if a=left)\n",
        "        0 (if a=otherwise)\n",
        "\n",
        "8. 에피소드: 에이전트가 처음시작상태에서 마지막상태까지 거치는 것.\n",
        "\n",
        "------------------------------------------------------------------------------ \n",
        "MDP(markov decision proess): 순차적인 행동을 알아내는 방법을 수학적으로 정의된 것. 상태, 행동, 보상함수, 상태변환확률, 감가율로 구성된다.\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "9. 감가율(감마): 0~1사이의 값. 미래의 보상에 가중치라고 생각. 스텝이 진행될수록 감가율을 제곱해 각각의 현재 보상과 곱해준다. 모두 합하면 반환값(G)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn-r5NxA2E0M",
        "colab_type": "text"
      },
      "source": [
        "##3. 강화학습의 방법\n",
        "강화학습 방법: 강화학습은 에피소드가 끝났을 때 에이전트가 거친 상태,보상, 행동을 기록한다.(\"음 상태를 봐서 이런 행동을 하니 이정도의 보상을 받는군\") 그 정보를 이용해 다음 에피소드에 대한 의사결정을 한다. 또 이 에피소드가 끝나면 이 에피소드를 통해 얻게 된 정보로 기록을 업데이트한다.\n",
        "이때 모든 보상을 합친 정보도 기록한다.\n",
        "\n",
        "__강화학습의 방법을 좀더 세세하게 들여다 보자.__\n",
        "\n",
        "로봇이 C라는 지점으로 이동한다고 생각해보자.\n",
        "\n",
        "1. 로봇은 처음 상태에서 에이전트는 환경을 관측해 얻은 관측값, 보상, 선택지(어떤 행동을 할 것인지)를 기반으로 이동(행동)한다. - 1단계(스텝) -\n",
        "2. 그 다음 상태에서 에이전트는 환경을 관측해 얻은 관측값, 보상, 선택지를 기반으로 이동한다. - 2단계 -\n",
        "3. C지점으로 이동할 때까지 매 스텝에서 이런 식으로 행동한다.(즉, 어떻게든 C지점으로 이동한다.)\n",
        "\n",
        "이걸 한 에피소드라고 한다.\n",
        "\n",
        "하지만 로봇이 C지점에 도착했다고 해서 효율적으로(최단거리로) 이동했다고 할 수 있을까? 아닐 수도 있다. 왜냐하면 로봇은 미래를 못보고 현재상태로만 따져서 현재상태에서는 좋은 행동을 할 수는 있어도 전체적으로 본다면 나쁜 행동일 수도 있기 때문이다.\n",
        "\n",
        "그래서 여기에 감가율을 도입한다.\n",
        "매 단계가 높아질수록 매 단계에서 얻은 보상을 1제곱을 높인 감가율(r)과 곱해준다. \n",
        "마지막 스텝까지 더한다.그리고 모든 단계에서 얻은 값을 더한다.\n",
        "\n",
        "G= a0 + a1 x r + a2 x (r^2) + a3 x (r^3) +... \n",
        "\n",
        "이건 설명하기가 어려우니 [pre-course](http://precourse.gj-aischool.com/lectures/26)를 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY0t1I12xIhE",
        "colab_type": "text"
      },
      "source": [
        "##4. 탐험과 활용\n",
        "\n",
        "강화학습은 많은 보상을 얻기 위해 최적의 정책을 내는 것이 목표.\n",
        "\n",
        "1. 탐험: 에이전트에게 무작위로 움직이라고 하는 것, __모든 행동__을 탐험한다.\n",
        "\n",
        "2. 활용(이용): 에이전트가 익숙한 움직임으로 즉, 찾아놓은 길로 하여 계속해서 선택하고 움직이는것. 기본적인 방법 중하나는 탐욕적 방법(가장 보상이 많이 얻는 행동을 하는 것)\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "__무작위 탐색 방법(탐험)__: \n",
        "\n",
        "에이전트가 취할 수 있는 행동 중 하나를 __임의__로 선택하는 방법. 에이전트가 다양한 경험을 하게 돼서 전체적으로 보상합이 가장 많은 행동을 취할 수 있게 된다.\n",
        "\n",
        "하지만 에이전트가 너무 다양한 경험만 추구하는 것은 좋은 방향이 아니다. 많은 상태와 행동이 존재하는 환경에서는 에이전트가 모든 경험을 하기엔 시간이 너무 많이 걸린다.\n",
        "\n",
        "그래서 어느 정도는 에이전트가 학습한 대로 행동 하는 것도 필요하다. 즉, 활용이 필요하다.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "__탐욕적 방법(활용)__: \n",
        "\n",
        "주어진 시점에 에이전트가 가장 큰 보상을 줄 것이라고 기대하는 행동만을 선택하는 것. 예를 들면 로봇이 상하좌우 방향의 보상값을 알고 있을때 가장 큰 보상을 가지는 행동만을 선택한다. \n",
        "\n",
        "그 상태에서는 활용이 바람직하지만(왜냐하면 보상을 많이주니까)\n",
        "\n",
        "전체적으로 바람직하지 않을 수 있다.(왜냐하면 그 상태에서만 보상을 많이 주고 연결된 나머지 다른 상태에서는 보상이 적어 보상의 총합은 오히려 적을 수 있다.)\n",
        "\n",
        "하지만 눈앞에 더 큰 보상을 쫒는 것이 더 매력적일 수밖에 없고 탐험을 하기에는 불확실할 수 밖에 없다.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "만약 강화학습을 하게 될때,\n",
        "\n",
        "- 탐험을 포함하지 않고 활용(탐욕적 방법)만 한다면 단기적으로는 큰 보상을 얻을 수 있다.\n",
        "\n",
        "- 탐험도 포함하면 단기적으로는 작은 보상을 얻지만 장기적으로는 더 큰 보상을 얻을 수있다.\n",
        "\n",
        "이렇게 탐험과 활용은 단점,장점이 있지만,하나의 행동을 할때 활용과 탐험을 동시에 할 수 없다. 이를 __갈등__(딜레마)이라고 불린다.\n",
        "\n",
        "그러면 행동을 할때 어떻게 활용 또는 탐험을 선택할까?\n",
        "\n",
        " 정밀한 가치 추정 값과 불확실성, 앞으로 남아있는 단계의 개수에 따라 결정하는 방법들이 있지만 대부분 사전 지식에 대한 가정을 기반으로 수립되는데 이러한 가정이 강화학습에 있어서 성립되지 않는다. 성립한다해도 정말로 성립하는지 검증할 방법이 없다.\n",
        "\n",
        " __그래서 어떻게 활용과 탐험을 선택하냐고!!!!__\n",
        "\n",
        " 명확한 답이 정해져 있지는 않는다. \n",
        " 하지만 모든 행동을 활용만 하거나 탐험만 하면 안된다. \n",
        " 활용과 탐험을 적절하게 분배해 사용해야 한다는 사실은 명확하다.\n",
        "\n",
        " 그래서 아래에 적절하게 분배하는 방법을 서술하겠다.\n",
        "\n",
        "1. 행동 가치방법: 행동의 가치를 추정하고 추정 값으로 부터 행동을 선택하도록 결정하는 방법.\n",
        "  추정하는 방법은 실제로 받았던 이전에 보상의 산술평균을 계산하는 것.\n",
        "\n",
        "  __행동이 갖는 가치의 참값__=\n",
        "\n",
        "  __행동이 선택될 때의 평균보상__=\n",
        "\n",
        "  _시각 t이전에 취해지는 행동(a)에 대한 보상의 합 __/__ 시각 t이전에 행동 a를 취하는 횟수._ \n",
        "\n",
        "  -- 정확한 수식은 pre-course 강화학습2 6:18에 나온다.\n",
        "\n",
        "  - 이 수식의 분모가 0이어서 계산을 할 수 없을때는 0과 같은 어떠한 기본값으로 정의.\n",
        "\n",
        "  - 분모가 무한대로 커지면 큰 수의 법칙에 따라 행동의 실제 가치로 접근한다.\n",
        "\n",
        "\n",
        "__가장 간단한 행동 선택 방법__: 모든 행동을 탐욕적 방법으로 선택. 즉, 눈앞(현재의 지식)의 보상만 따져서 선택. 일절 탐험을 하지 않는다.\n",
        "\n",
        "__입실론-탐욕적 방법__: 이런 선택규칙방법을 대안하는 방법은 \n",
        "입실론-탐욕적 방법이다. \n",
        "\n",
        "대부분 탐욕적 방법(활용)을 사용하고 아주 가끔 탐험한다. 이때 모든 행동이 선택될 확률은 균등하고 행동 선택은 행동가치 추정과는 무관하게 이루어진다.\n",
        "\n",
        "효용성은 미지수이다.\n"
      ]
    }
  ]
}